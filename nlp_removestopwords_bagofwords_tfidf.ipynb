{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP61SEy6UJgaSR8NTtJwV7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudama-inc/nlp/blob/main/nlp_removestopwords_bagofwords_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remove Stop Words**\n",
        "> Libraries\n",
        "*   NLTK\n",
        "*   SpaCy\n",
        "*   Sklearn"
      ],
      "metadata": {
        "id": "yYYtSL6R0hQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Stop Words : **NLTK**"
      ],
      "metadata": {
        "id": "9CiMvtCd3CrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "MT0nnU530nes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_9A9vyd0nhF",
        "outputId": "7869f589-59ad-4176-c75e-4e5fbd0ed447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"This is an example sentence with some stopwords.\""
      ],
      "metadata": {
        "id": "NGnuPLUd0njp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hodyHCr0nmF",
        "outputId": "84d8e151-8c67-42d4-8187-d86096132050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This', 'is', 'an', 'example', 'sentence', 'with', 'some', 'stopwords', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of English stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYDdSTut0noS",
        "outputId": "34bc1f84-c6a1-417e-f76a-8cf32cdad580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'wouldn', 'whom', 'below', 'few', 'from', 'that', 'was', 'at', 'than', 'ain', 'an', 'her', \"haven't\", 'can', 'aren', 'nor', 'how', 'has', 'don', 'in', \"aren't\", \"hadn't\", 'to', 'before', \"mightn't\", 'him', 'been', 'where', 'yourselves', 'other', 'on', 'my', 'be', \"shouldn't\", 'when', 'above', 'up', 'y', 'haven', 'who', 'yourself', \"you've\", 'should', 'further', 'between', \"hasn't\", \"needn't\", 'down', 'by', 'once', 'with', \"won't\", \"it's\", 'they', 'hasn', 'she', 'he', 'our', 'did', \"she's\", 'but', 'or', 'there', \"you'd\", 'ma', 'just', 'were', 'having', 'is', 'during', \"doesn't\", 'mustn', 'about', 'off', 'will', 'didn', 'through', \"should've\", 'o', 'each', 'do', 'too', 'it', 'this', 'isn', 'for', 'their', 'only', 'have', 'of', \"wasn't\", 'more', 'here', 'those', 'own', 's', \"wouldn't\", \"you're\", 'out', 'its', 'what', 'these', 'any', 'until', 'd', 'doing', \"mustn't\", 'shan', 'ourselves', 'why', 've', 'doesn', 'won', 'you', 'into', 'couldn', \"that'll\", \"you'll\", 'which', 'am', 'shouldn', 'theirs', 'itself', \"shan't\", \"couldn't\", 'not', 'mightn', 'and', 'all', 't', 'then', 'are', 'me', 'under', \"weren't\", \"didn't\", 'same', 'now', 'a', 'll', 'ours', 'hers', 'his', 'as', 'we', 'against', 'them', \"isn't\", 'both', 'while', 'very', 'herself', 'yours', 'again', 'had', 'most', 'i', 'weren', 'hadn', 'needn', \"don't\", 'because', 'himself', 'wasn', 'no', 'm', 'the', 'does', 're', 'if', 'some', 'your', 'being', 'after', 'such', 'over', 'themselves', 'myself', 'so'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords from the tokenized words\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "filtered_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWfh2q2m0nqr",
        "outputId": "722be64c-ba88-40c5-ba46-524607a68274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example', 'sentence', 'stopwords', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the filtered words back into a sentence\n",
        "filtered_text = \" \".join(filtered_words)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhEC4Btb0nth",
        "outputId": "e24ba830-d663-4e94-8e6c-7b48083a1b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example sentence stopwords .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPJLuu6P0nvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Stop Words : **SpaCy**"
      ],
      "metadata": {
        "id": "E-HH67813KV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "gbxOCLuM0nyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy language model with stopwords\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFcvhUhT5Lo3",
        "outputId": "5df329d9-a7de-429e-dcf7-e5be1735ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7c56694b72e0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "text = \"This is an example sentence with some stopwords.\""
      ],
      "metadata": {
        "id": "TNdhW-fm5TQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEGfz3KT5WFd",
        "outputId": "15bd778c-2b87-4241-8f3c-f23190689274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "This is an example sentence with some stopwords."
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and join the remaining words\n",
        "filtered_text_spacy = \" \".join([token.text for token in doc if not token.is_stop])\n",
        "print(filtered_text_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkiPqIaT5ayl",
        "outputId": "4d1dfe20-646f-4b65-e539-3993fe8a4b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example sentence stopwords .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MsnA9i6B5bPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Stop Words : **Using scikit-learn's CountVectorizer**"
      ],
      "metadata": {
        "id": "o-bZCJGG6D--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "Nhzw6dPu5bR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "documents = [\"This is an example sentence with some stopwords.\", \"Another example with stopwords.\"]"
      ],
      "metadata": {
        "id": "Zm0ntfE95bUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of CountVectorizer with stop words removed\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "vectorizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "1meLTPfS5bWt",
        "outputId": "f3ffb812-b0e5-44dc-c11a-b91f456cf1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(stop_words='english')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the vectorizer on your text data and transform the text into a document-term matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "X.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o6Y15Gj6vGO",
        "outputId": "0eee3276-a59a-4011-a156-5981535fa401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1],\n",
              "       [1, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of terms (words)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "terms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gvsjmrH6vIo",
        "outputId": "2bdb067b-b9d2-490d-eb37-f666465c67a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['example', 'sentence', 'stopwords'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4yFP8466vLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-yG049G8ouB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**\n",
        "> Libraries\n",
        "*   NLTK\n",
        "*   SpaCy\n",
        "*   Sklearn"
      ],
      "metadata": {
        "id": "17kA3epL8o2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Bag-of-Words : **Using scikit-learn's CountVectorizer**"
      ],
      "metadata": {
        "id": "FDRbKggM8vRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on your text data and transform the text into a document-term matrix\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# X is a sparse matrix where rows represent documents, and columns represent terms\n",
        "# You can convert it to a dense array for better readability\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Get the list of terms (words)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the document-term matrix\n",
        "print(\"Document-Term Matrix:\")\n",
        "print(X_dense)\n",
        "print(\"-\"*30)\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdQszHrc5bZk",
        "outputId": "34c53413-ec18-45bb-a6e0-8b43e715660f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-Term Matrix:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n",
            "------------------------------\n",
            "List of Terms (Words):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Bag-of-Words : **Using NLTK**"
      ],
      "metadata": {
        "id": "Lln6aFVQ9Ltj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Sample text data\n",
        "text = \"This is a simple example for creating a Bag of Words representation using NLTK.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Calculate word frequencies\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Get the list of terms (words)\n",
        "terms = list(freq_dist.keys())\n",
        "\n",
        "# Print the document-term matrix (word frequencies)\n",
        "print(\"Document-Term Matrix (Word Frequencies):\")\n",
        "for term in terms:\n",
        "    print(f\"{term}: {freq_dist[term]}\")\n",
        "\n",
        "print(\"-\"*30)\n",
        "\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJw4Vwgh7_3t",
        "outputId": "a9abbabb-47c2-4166-eb8d-7b7aa226eb2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document-Term Matrix (Word Frequencies):\n",
            "This: 1\n",
            "is: 1\n",
            "a: 2\n",
            "simple: 1\n",
            "example: 1\n",
            "for: 1\n",
            "creating: 1\n",
            "Bag: 1\n",
            "of: 1\n",
            "Words: 1\n",
            "representation: 1\n",
            "using: 1\n",
            "NLTK: 1\n",
            ".: 1\n",
            "------------------------------\n",
            "List of Terms (Words):\n",
            "['This', 'is', 'a', 'simple', 'example', 'for', 'creating', 'Bag', 'of', 'Words', 'representation', 'using', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Bag-of-Words : **Using SpaCy**"
      ],
      "metadata": {
        "id": "OC33gZwQ9aZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a simple example for creating a Bag of Words representation using spaCy.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Get the list of terms (lemmatized words) without stopwords\n",
        "terms = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ElhI5MI8PMj",
        "outputId": "1fbd910b-9efe-4628-e08c-de2dc43506a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of Terms (Words):\n",
            "['simple', 'example', 'create', 'Bag', 'Words', 'representation', 'spacy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCy5Nh2O8Vsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH9XMLud-E2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**\n",
        "> Libraries\n",
        "*   NLTK\n",
        "*   SpaCy\n",
        "*   Sklearn"
      ],
      "metadata": {
        "id": "WhST-nYD-Fo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   TF-IDF : **Using scikit-learn's TfidfVectorizer**"
      ],
      "metadata": {
        "id": "BzW6ZPq-BTj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vertorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on your text data and transform the text into a TF-IDF matrix\n",
        "X = vertorizer.fit_transform(documents)\n",
        "\n",
        "# X is a sparse matrix where rows represent documents, and columns represent terms\n",
        "# You can convert it to a dense array for better readability\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Get the list of terms (words)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X_dense)\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvGo4M-c-E5Y",
        "outputId": "5081c920-6206-4c18-81ca-2ebfc88cc889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "--------------------------------------------------\n",
            "List of Terms (Words):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7G6GQD85-E7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   TF-IDF : **Using NLTK & scikit-learn**"
      ],
      "metadata": {
        "id": "OWlrr_6yBa7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Sample text data\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Tokenize the text using NLTK\n",
        "\n",
        "tokenized_documents = [' '.join(word_tokenize(doc)) for doc in documents]\n",
        "print(tokenized_documents)\n",
        "\n",
        "# Fit the vectorizer on your tokenized text data and transform it into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(tokenized_documents)\n",
        "\n",
        "# X is a sparse matrix where rows represent documents, and columns represent terms\n",
        "# You can convert it to a dense array for better readability\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Get the list of terms (words)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X_dense)\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7YFCt0B-E_3",
        "outputId": "ba085b5f-f663-4c3b-f932-98b9ba1953a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is the first document .', 'This document is the second document .', 'And this is the third one .', 'Is this the first document ?']\n",
            "--------------------------------------------------\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n",
            "--------------------------------------------------\n",
            "List of Terms (Words):\n",
            "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A3zQb1T1-FCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.   Bag-of-Words : **Using SpaCy & scikit-learn**"
      ],
      "metadata": {
        "id": "1caboiZ_CnzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "]\n",
        "\n",
        "# Process the text with spaCy and extract lemmatized words\n",
        "tokenized_documents = [' '.join([token.lemma_ for token in nlp(doc) if not token.is_stop]) for doc in documents]\n",
        "print('Tokenized Documents : ')\n",
        "print(tokenized_documents)\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on your tokenized text data and transform it into a TF-IDF matrix\n",
        "X = vectorizer.fit_transform(tokenized_documents)\n",
        "\n",
        "# X is a sparse matrix where rows represent documents, and columns represent terms\n",
        "# You can convert it to a dense array for better readability\n",
        "X_dense = X.toarray()\n",
        "\n",
        "# Get the list of terms (words)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(X_dense)\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Print the list of terms\n",
        "print(\"List of Terms (Words):\")\n",
        "print(terms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTwMcdjaCmD7",
        "outputId": "dc865a11-c345-4b92-f35c-3e91df54b307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Documents : \n",
            "['document .', 'document second document .', '.', 'document ?']\n",
            "--------------------------------------------------\n",
            "TF-IDF Matrix:\n",
            "[[1.         0.        ]\n",
            " [0.78722298 0.61666846]\n",
            " [0.         0.        ]\n",
            " [1.         0.        ]]\n",
            "--------------------------------------------------\n",
            "List of Terms (Words):\n",
            "['document' 'second']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ReZgbwlLCmG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0X9iyATcCmJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming & Lemmatization**\n",
        "> Libraries\n",
        "*   NLTK\n",
        "*   SpaCy"
      ],
      "metadata": {
        "id": "2VNyqRw5FRnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Stemming & Lemmatization : **Using NLTK**"
      ],
      "metadata": {
        "id": "12WIQtGSFZ9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Sample text data\n",
        "text = \"Stemming and lemmatization are text processing techniques used for text analysis.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "\n",
        "print('-'*50)\n",
        "\n",
        "# Stemming using the Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "print(\"Stemming using the Porter Stemmer : \")\n",
        "print(stemmed_words)\n",
        "print('-'*50)\n",
        "\n",
        "# Lemmatization using the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Lemmatization using the WordNet Lemmatizer : \")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzCBhpQ_CmL3",
        "outputId": "a6f17ed0-053d-4b3d-8dac-e946718bb4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Stemming', 'and', 'lemmatization', 'are', 'text', 'processing', 'techniques', 'used', 'for', 'text', 'analysis', '.']\n",
            "--------------------------------------------------\n",
            "Stemming using the Porter Stemmer : \n",
            "['stem', 'and', 'lemmat', 'are', 'text', 'process', 'techniqu', 'use', 'for', 'text', 'analysi', '.']\n",
            "--------------------------------------------------\n",
            "Lemmatization using the WordNet Lemmatizer : \n",
            "['Stemming', 'and', 'lemmatization', 'are', 'text', 'processing', 'technique', 'used', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYemvO5f-FFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.   Lemmatization : **Using SpaCy**"
      ],
      "metadata": {
        "id": "yHsCtOvUJ9BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Stemming and lemmatization are text processing techniques used for text analysis.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Lemmatization using spaCy\n",
        "lemmatized_words = [tiken.lemma_ for tiken in doc]\n",
        "\n",
        "# Print the lemmatized words\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9p6RQGcKAOs",
        "outputId": "9bd7915e-cdfc-4534-e53f-ad5cb2c94467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: ['stemming', 'and', 'lemmatization', 'be', 'text', 'processing', 'technique', 'use', 'for', 'text', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlOWLfixKARX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}